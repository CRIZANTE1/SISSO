import pandas as pd
import numpy as np
from typing import List, Optional, Dict, Any
from managers.supabase_config import get_supabase_client
import streamlit as st

# Import scipy opcionalmente
try:
    from scipy import stats
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("‚ö†Ô∏è SciPy n√£o est√° dispon√≠vel. Algumas funcionalidades estat√≠sticas podem estar limitadas.")

# Escala das horas: dados cadastrados em centenas (ex: 176 representa 17.600 horas)
HOURS_SCALE = 100

def fetch_kpi_data(user_email: Optional[str] = None,
                   start_date: Optional[str] = None, 
                   end_date: Optional[str] = None) -> pd.DataFrame:
    """Busca dados de KPI do Supabase"""
    try:
        from auth.auth_utils import get_user_id, is_admin
        from managers.supabase_config import get_service_role_client
        
        user_id = get_user_id()
        if not user_id:
            return pd.DataFrame()
        
        # Usa service_role para contornar RLS e aplicar filtro de seguran√ßa no c√≥digo
        supabase = get_service_role_client()
        
        query = supabase.table("kpi_monthly").select("*")
        
        # Admin v√™ todos os dados sem filtro de created_by
        if not is_admin():
            # Usu√°rio comum v√™ apenas seus pr√≥prios KPIs
            query = query.eq("created_by", user_id)
        
        if start_date:
            query = query.gte("period", start_date)
        if end_date:
            query = query.lte("period", end_date)
            
        response = query.order("period").execute()
        
        if response and hasattr(response, 'data'):
            df = pd.DataFrame(response.data) if response.data else pd.DataFrame()
            
            # Valida√ß√£o adicional de seguran√ßa para usu√°rios n√£o-admin
            if not is_admin() and not df.empty:
                # Filtra novamente para garantir (seguran√ßa em camadas)
                df = df[df['created_by'] == user_id]
            
            return df
        return pd.DataFrame()
    except Exception as e:
        st.error(f"Erro ao buscar dados de KPI: {str(e)}")
        import traceback
        st.code(traceback.format_exc())
        return pd.DataFrame()

def calculate_frequency_rate(accidents: int, hours_worked: float) -> float:
    """
    Calcula Taxa de Frequ√™ncia (TF) conforme NBR 14280
    
    F√≥rmula: (N¬∞ de acidentes x 1.000.000) / hora-homem trabalhada
    
    A TF indica a quantidade de acidentes ocorridos numa empresa em fun√ß√£o 
    da exposi√ß√£o ao risco (horas-homem de exposi√ß√£o ao risco).
    
    Par√¢metros de refer√™ncia:
    - TF at√© 20 = muito bom
    - 20,1-40 = bom  
    - 40,1-60 = ruim
    - acima de 60 = p√©ssima
    """
    if hours_worked is None or hours_worked == 0:
        return 0.0
    return (accidents / (hours_worked * HOURS_SCALE)) * 1_000_000

def calculate_severity_rate(lost_days: int, hours_worked: float, debited_days: int = 0) -> float:
    """
    Calcula Taxa de Gravidade (TG) conforme NBR 14280
    
    F√≥rmula: ((dias perdidos + dias debitados) x 1.000.000) / hora-homem trabalhada
    
    A TG mede o impacto ou severidade dos acidentes em termos de tempo de trabalho 
    perdido (ou que ser√° perdido) por cada milh√£o de horas-homem de exposi√ß√£o ao risco.
    
    Args:
        lost_days: Dias que o trabalhador ficou afastado por conta do acidente
        hours_worked: Total de horas-homem trabalhadas
        debited_days: Dias debitados para casos graves (incapacidade permanente, morte)
                      Ex: morte = 6000 dias, amputa√ß√£o de m√£o = 3000 dias
    """
    if hours_worked is None or hours_worked == 0:
        return 0.0
    return ((lost_days + debited_days) / (hours_worked * HOURS_SCALE)) * 1_000_000

def get_frequency_rate_interpretation(tf_value: float) -> dict:
    """
    Retorna interpreta√ß√£o da Taxa de Frequ√™ncia conforme par√¢metros de refer√™ncia
    """
    if tf_value <= 20:
        return {
            "classification": "Muito Bom",
            "color": "success",
            "icon": "‚úÖ",
            "description": "Excelente desempenho em preven√ß√£o de acidentes"
        }
    elif tf_value <= 40:
        return {
            "classification": "Bom", 
            "color": "info",
            "icon": "üìä",
            "description": "Bom desempenho, manter pr√°ticas atuais"
        }
    elif tf_value <= 60:
        return {
            "classification": "Ruim",
            "color": "warning", 
            "icon": "‚ö†Ô∏è",
            "description": "Desempenho ruim, revisar procedimentos de seguran√ßa"
        }
    else:
        return {
            "classification": "P√©ssimo",
            "color": "danger",
            "icon": "üö®", 
            "description": "Desempenho cr√≠tico, a√ß√£o imediata necess√°ria"
        }

def get_severity_rate_interpretation(tg_value: float) -> dict:
    """
    Retorna interpreta√ß√£o da Taxa de Gravidade
    """
    if tg_value <= 50:
        return {
            "classification": "Excelente",
            "color": "success",
            "icon": "‚úÖ",
            "description": "Baixo impacto dos acidentes"
        }
    elif tg_value <= 100:
        return {
            "classification": "Aceit√°vel",
            "color": "info", 
            "icon": "üìä",
            "description": "Impacto moderado, monitorar"
        }
    elif tg_value <= 200:
        return {
            "classification": "Elevado",
            "color": "warning",
            "icon": "‚ö†Ô∏è",
            "description": "Alto impacto, revisar medidas preventivas"
        }
    else:
        return {
            "classification": "Cr√≠tico",
            "color": "danger",
            "icon": "üö®",
            "description": "Impacto cr√≠tico, investiga√ß√£o imediata necess√°ria"
        }

def calculate_poisson_control_limits(df: pd.DataFrame, 
                                   accidents_col: str = 'accidents_total',
                                   hours_col: str = 'hours') -> pd.DataFrame:
    """Calcula limites de controle Poisson para eventos raros"""
    df = df.copy()
    
    # Taxa m√©dia de acidentes por hora
    total_accidents = df[accidents_col].sum()
    total_hours = df[hours_col].sum()
    total_hours_corrected = total_hours * HOURS_SCALE
    mean_rate = total_accidents / total_hours_corrected if total_hours_corrected > 0 else 0
    
    # Valor esperado para cada per√≠odo
    df['expected'] = (df[hours_col] * HOURS_SCALE) * mean_rate
    
    # Limites de controle (3-sigma)
    df['ucl'] = df['expected'] + 3 * np.sqrt(df['expected'])
    df['lcl'] = np.maximum(0, df['expected'] - 3 * np.sqrt(df['expected']))
    
    # Identificar pontos fora de controle
    df['out_of_control'] = (df[accidents_col] > df['ucl']) | (df[accidents_col] < df['lcl'])
    
    return df

def calculate_ewma(df: pd.DataFrame, 
                  value_col: str, 
                  lambda_param: float = 0.2) -> pd.DataFrame:
    """Calcula EWMA (Exponentially Weighted Moving Average)"""
    df = df.copy()
    df = df.sort_values('period')
    
    # Inicializa com a m√©dia dos primeiros valores
    initial_mean = df[value_col].head(12).mean() if len(df) >= 12 else df[value_col].mean()
    
    ewma_values = []
    ewma = initial_mean
    
    for value in df[value_col]:
        ewma = lambda_param * value + (1 - lambda_param) * ewma
        ewma_values.append(ewma)
    
    df['ewma'] = ewma_values
    
    # Limites de controle para EWMA
    variance = df[value_col].var()
    sigma_ewma = np.sqrt((lambda_param / (2 - lambda_param)) * variance)
    
    df['ewma_ucl'] = initial_mean + 3 * sigma_ewma
    df['ewma_lcl'] = initial_mean - 3 * sigma_ewma
    
    return df

def detect_control_chart_patterns(df: pd.DataFrame, 
                                value_col: str, 
                                ucl_col: str, 
                                lcl_col: str) -> Dict[str, List[int]]:
    """Detecta padr√µes de controle estat√≠stico"""
    patterns = {
        'out_of_control': [],
        'trend_up': [],
        'trend_down': [],
        'runs_above_center': [],
        'runs_below_center': []
    }
    
    values = df[value_col].values
    ucl_values = df[ucl_col].values
    lcl_values = df[lcl_col].values
    center_line = values.mean()
    
    for i in range(len(values)):
        # Pontos fora de controle
        if values[i] > ucl_values[i] or values[i] < lcl_values[i]:
            patterns['out_of_control'].append(i)
        
        # Tend√™ncia ascendente (8 pontos consecutivos)
        if i >= 7:
            if all(values[j] > values[j-1] for j in range(i-7, i+1)):
                patterns['trend_up'].append(i)
        
        # Tend√™ncia descendente (8 pontos consecutivos)
        if i >= 7:
            if all(values[j] < values[j-1] for j in range(i-7, i+1)):
                patterns['trend_down'].append(i)
    
    return patterns

def generate_kpi_summary(df: pd.DataFrame) -> Dict[str, Any]:
    """Gera resumo dos KPIs com interpreta√ß√µes conforme NBR 14280 e ISO 45001"""
    if df.empty:
        return {}
    
    # Calcula totais acumulados para todo o per√≠odo
    total_accidents = df['accidents_total'].sum()
    total_lost_days = df['lost_days_total'].sum()
    total_hours = df['hours'].sum()
    total_hours_corrected = total_hours * HOURS_SCALE
    total_fatalities = df.get('fatalities', pd.Series([0] * len(df))).sum()
    total_debited_days = df.get('debited_days', pd.Series([0] * len(df))).sum()
    
    # Calcula automaticamente os dias debitados para acidentes fatais conforme NBR 14280
    # Morte = 6.000 dias debitados
    automatic_debited_days = total_fatalities * 6000
    total_debited_days = total_debited_days + automatic_debited_days
    
    # ‚úÖ VALIDA√á√ÉO: Verifica se h√° horas v√°lidas
    if total_hours_corrected <= 0:
        st.warning("‚ö†Ô∏è Total de horas trabalhadas √© zero ou inv√°lido. Verifique os dados cadastrados.")
        return {
            'frequency_rate': 0,
            'severity_rate': 0,
            'total_accidents': total_accidents,
            'total_fatalities': total_fatalities,
            'total_lost_days': total_lost_days,
            'total_hours': 0,
            'error': 'Horas inv√°lidas'
        }
    
    # ‚úÖ C√°lculos ACUMULADOS para todo o per√≠odo (correto para vis√£o geral)
    freq_rate = calculate_frequency_rate(total_accidents, total_hours)
    sev_rate = calculate_severity_rate(total_lost_days, total_hours, total_debited_days)
    
    # ‚úÖ NOVO: Calcula tamb√©m taxas POR PER√çODO para an√°lise mais precisa
    df_with_rates = df.copy()
    df_with_rates['freq_rate_period'] = df_with_rates.apply(
        lambda row: calculate_frequency_rate(
            row['accidents_total'], 
            row['hours']
        ) if row['hours'] > 0 else 0, 
        axis=1
    )
    
    # Calcula dias debitados por per√≠odo (fatalidades * 6000)
    df_with_rates['period_debited_days'] = (
        df_with_rates.get('debited_days', 0) + 
        (df_with_rates.get('fatalities', 0) * 6000)
    )
    
    df_with_rates['sev_rate_period'] = df_with_rates.apply(
        lambda row: calculate_severity_rate(
            row['lost_days_total'], 
            row['hours'],
            row['period_debited_days']
        ) if row['hours'] > 0 else 0, 
        axis=1
    )
    
    # ‚úÖ CORRIGIDO: Taxa acumulada √© sempre a forma correta conforme NBR 14280
    # A "m√©dia por per√≠odo" n√£o faz sentido estat√≠stico quando horas variam entre per√≠odos
    # O padr√£o da norma √©: (Total de acidentes / Total de horas) √ó 1.000.000
    # 
    # Calculamos taxas por per√≠odo apenas para an√°lise de tend√™ncias, n√£o para exibi√ß√£o principal
    
    # Para m√∫ltiplos per√≠odos, usa a TAXA ACUMULADA (padr√£o da norma)
    # A taxa acumulada j√° √© a forma correta de calcular quando h√° m√∫ltiplos per√≠odos
    avg_freq_rate = float(freq_rate)  # Na verdade √© taxa acumulada
    avg_sev_rate = float(sev_rate)    # Na verdade √© taxa acumulada
    
    # ‚úÖ SEMPRE usa taxa acumulada para interpreta√ß√£o (padr√£o NBR 14280)
    freq_interpretation = get_frequency_rate_interpretation(float(freq_rate))
    sev_interpretation = get_severity_rate_interpretation(float(sev_rate))
    
    # ‚úÖ MELHORADO: Compara√ß√£o com m√©dia dos √∫ltimos 3 per√≠odos (mais est√°vel)
    freq_change = None
    sev_change = None
    
    if len(df_with_rates) >= 4:
        # Ordena por per√≠odo
        df_sorted = df_with_rates.sort_values('period')
        
        # √öltimo per√≠odo
        latest_freq = df_sorted['freq_rate_period'].iloc[-1]
        latest_sev = df_sorted['sev_rate_period'].iloc[-1]
        
        # M√©dia dos 3 per√≠odos anteriores
        prev_3_freq = df_sorted['freq_rate_period'].iloc[-4:-1].mean()
        prev_3_sev = df_sorted['sev_rate_period'].iloc[-4:-1].mean()
        
        # Calcula varia√ß√£o percentual
        if prev_3_freq > 0:
            freq_change = ((latest_freq - prev_3_freq) / prev_3_freq * 100)
        
        if prev_3_sev > 0:
            sev_change = ((latest_sev - prev_3_sev) / prev_3_sev * 100)
    
    elif len(df_with_rates) >= 2:
        # Se tiver apenas 2-3 per√≠odos, compara com o anterior
        df_sorted = df_with_rates.sort_values('period')
        
        latest_freq = df_sorted['freq_rate_period'].iloc[-1]
        latest_sev = df_sorted['sev_rate_period'].iloc[-1]
        
        prev_freq = df_sorted['freq_rate_period'].iloc[-2]
        prev_sev = df_sorted['sev_rate_period'].iloc[-2]
        
        if prev_freq > 0:
            freq_change = ((latest_freq - prev_freq) / prev_freq * 100)
        
        if prev_sev > 0:
            sev_change = ((latest_sev - prev_sev) / prev_sev * 100)
    
    # ‚úÖ ISO 45001: Adicionando m√©tricas de desempenho e an√°lise de tend√™ncias
    # Conformidade com requisitos ISO 45001:2018
    iso_compliance_metrics = calculate_iso_compliance_metrics(df_with_rates)
    
    # ‚úÖ NBR 14280: An√°lise de tend√™ncias de acidentes
    accident_trend_analysis = analyze_accident_trends(df_with_rates)
    
    return {
        'latest_period': df['period'].max(),
        
        # Taxas acumuladas (total do per√≠odo)
        'frequency_rate': freq_rate,
        'severity_rate': sev_rate,
        
        # ‚úÖ NOVO: Taxas m√©dias por per√≠odo
        'avg_frequency_rate': avg_freq_rate,
        'avg_severity_rate': avg_sev_rate,
        
        # Varia√ß√µes
        'frequency_change': freq_change,
        'severity_change': sev_change,
        
        # Totalizadores
        'total_accidents': int(total_accidents),
        'total_fatalities': int(total_fatalities),
        'total_lost_days': int(total_lost_days),
        'total_debited_days': int(total_debited_days),
        'automatic_debited_days': int(automatic_debited_days),
        'total_hours': float(total_hours_corrected),
        
        # Interpreta√ß√µes
        'frequency_interpretation': freq_interpretation,
        'severity_interpretation': sev_interpretation,
        
        # ‚úÖ NOVO: Metadados para transpar√™ncia
        'periods_count': len(df),
        'calculation_method': 'accumulated' if len(df) > 1 else 'single_period',
        
        # ‚úÖ NOVO: M√©tricas de conformidade ISO 45001:2018
        'iso_compliance_metrics': iso_compliance_metrics,
        
        # ‚úÖ NOVO: An√°lise de tend√™ncias de acidentes conforme NBR 14280
        'accident_trend_analysis': accident_trend_analysis
    }

def calculate_iso_compliance_metrics(df_with_rates: pd.DataFrame) -> Dict[str, Any]:
    """
    Calcula m√©tricas de conformidade com ISO 45001:2018
    A ISO 45001 exige monitoramento cont√≠nuo, investiga√ß√£o de incidentes,
    implementa√ß√£o de a√ß√µes corretivas e melhoria cont√≠nua
    """
    metrics = {}
    
    # Avalia√ß√£o de melhoria cont√≠nua (tend√™ncia de melhoria nos KPIs)
    if len(df_with_rates) >= 2:
        recent_freq = df_with_rates['freq_rate_period'].tail(3)
        recent_sev = df_with_rates['sev_rate_period'].tail(3)
        
        # Melhoria cont√≠nua - se os KPIs est√£o em tend√™ncia de melhoria
        freq_trend = "improving" if len(recent_freq) >= 2 and recent_freq.iloc[-1] < recent_freq.iloc[-2] else "not_improving"
        sev_trend = "improving" if len(recent_sev) >= 2 and recent_sev.iloc[-1] < recent_sev.iloc[-2] else "not_improving"
        
        metrics['continuous_improvement'] = {
            'frequency_trend': freq_trend,
            'severity_trend': sev_trend,
            'is_improving': freq_trend == "improving" and sev_trend == "improving"
        }
    
    # Monitoramento e medi√ß√£o (conformidade com cl√°usula 9.1)
    metrics['monitoring_compliance'] = {
        'has_consistent_data': len(df_with_rates) > 0 and df_with_rates['accidents_total'].sum() >= 0,
        'data_quality_score': calculate_data_quality_score(df_with_rates)
    }
    
    # A√ß√µes corretivas e preventivas (conformidade com cl√°usula 10.2)
    # Isso seria implementado com base em dados de a√ß√µes corretivas registradas
    
    return metrics

def analyze_accident_trends(df_with_rates: pd.DataFrame) -> Dict[str, Any]:
    """
    An√°lise de tend√™ncias de acidentes conforme NBR 14280
    """
    if df_with_rates.empty:
        return {}
    
    trends = {}
    
    # Tend√™ncia de longo prazo (√∫ltimos 6 meses vs 6 meses anteriores)
    if len(df_with_rates) >= 12:
        recent_6 = df_with_rates.tail(6)['freq_rate_period'].mean()
        prev_6 = df_with_rates.head(6)['freq_rate_period'].mean()
        
        if prev_6 > 0:
            change_pct = ((recent_6 - prev_6) / prev_6) * 100
            trends['long_term_trend'] = {
                'change_percentage': change_pct,
                'direction': 'improving' if change_pct < 0 else 'worsening' if change_pct > 0 else 'stable'
            }
    
    # Tend√™ncia de curto prazo (√∫ltimos 3 meses)
    if len(df_with_rates) >= 6:
        recent_3 = df_with_rates.tail(3)['freq_rate_period'].mean()
        prev_3 = df_with_rates.iloc[-6:-3]['freq_rate_period'].mean()
        
        if prev_3 > 0:
            change_pct = ((recent_3 - prev_3) / prev_3) * 100
            trends['short_term_trend'] = {
                'change_percentage': change_pct,
                'direction': 'improving' if change_pct < 0 else 'worsening' if change_pct > 0 else 'stable'
            }
    
    # Varia√ß√£o coeficiente de varia√ß√£o (medida de estabilidade)
    if len(df_with_rates) >= 3:
        freq_cv = df_with_rates['freq_rate_period'].std() / df_with_rates['freq_rate_period'].mean() if df_with_rates['freq_rate_period'].mean() > 0 else 0
        trends['stability'] = {
            'coefficient_of_variation': freq_cv,
            'stability_level': 'high' if freq_cv < 0.2 else 'medium' if freq_cv < 0.5 else 'low'
        }
    
    return trends

def calculate_data_quality_score(df: pd.DataFrame) -> float:
    """
    Calcula um escore de qualidade dos dados
    """
    score = 100.0  # Pontua√ß√£o base
    
    # Verifica completude dos dados
    required_columns = ['accidents_total', 'hours', 'lost_days_total']
    for col in required_columns:
        if col in df.columns:
            completeness = df[col].notna().sum() / len(df) * 100
            score *= (completeness / 100)  # Reduz pontua√ß√£o proporcionalmente
    
    # Verifica consist√™ncia temporal
    if 'period' in df.columns:
        expected_periods = len(df)
        unique_periods = df['period'].nunique()
        consistency = (unique_periods / expected_periods) * 100
        score *= (consistency / 100)
    
    return max(0, min(100, score))  # Limita entre 0 e 100

def calculate_corrective_actions_metrics(df_with_rates: pd.DataFrame) -> Dict[str, Any]:
    """
    Calcula m√©tricas de efic√°cia de a√ß√µes corretivas e preventivas
    Conforme requisitos da ISO 45001:2018 cl√°usula 10.2
    """
    metrics = {}
    
    # Este c√°lculo seria baseado em dados de a√ß√µes corretivas do sistema
    # Por enquanto, implementando uma m√©trica baseada na redu√ß√£o de acidentes ap√≥s eventos
    if len(df_with_rates) >= 3:
        # Comparando m√©dia m√≥vel de acidentes
        recent_accidents = df_with_rates['accidents_total'].tail(3).mean()
        prev_accidents = df_with_rates['accidents_total'].head(3).mean()
        
        if prev_accidents > 0:
            improvement_rate = ((prev_accidents - recent_accidents) / prev_accidents) * 100
            metrics['corrective_effectiveness'] = {
                'improvement_rate': improvement_rate,
                'is_improving': improvement_rate > 0
            }
    
    return metrics

def generate_iso_45001_compliance_report(kpi_summary: Dict[str, Any]) -> List[str]:
    """
    Gera relat√≥rio de conformidade com ISO 45001:2018
    """
    report = []
    
    # Avalia√ß√£o das cl√°usulas principais da ISO 45001
    freq_rate = kpi_summary.get('frequency_rate', 0)
    sev_rate = kpi_summary.get('severity_rate', 0)
    
    # Cl√°usula 9.1 - Monitoramento, medi√ß√£o, an√°lise e avalia√ß√£o
    report.append("üìä **Cl√°usula 9.1 - Monitoramento e medi√ß√£o:**")
    report.append(f"   - Taxa de Frequ√™ncia: {freq_rate:.2f} acidentes/milh√£o de horas")
    report.append(f"   - Taxa de Gravidade: {sev_rate:.2f} dias perdidos/milh√£o de horas")
    
    # Cl√°usula 10 - Melhoria
    freq_change = kpi_summary.get('frequency_change', 0)
    sev_change = kpi_summary.get('severity_change', 0)
    
    improvement_status = []
    if freq_change is not None:
        improvement_status.append(f"Taxa de Frequ√™ncia: {'Melhorando' if freq_change < 0 else 'Piorando' if freq_change > 0 else 'Est√°vel'} ({freq_change:+.1f}%)")
    if sev_change is not None:
        improvement_status.append(f"Taxa de Gravidade: {'Melhorando' if sev_change < 0 else 'Piorando' if sev_change > 0 else 'Est√°vel'} ({sev_change:+.1f}%)")
    
    report.append("üîÑ **Cl√°usula 10 - Melhoria cont√≠nua:**")
    for status in improvement_status:
        report.append(f"   - {status}")
    
    # Cl√°usula 10.2 - A√ß√µes corretivas e preventivas
    report.append("üîß **Cl√°usula 10.2 - A√ß√µes corretivas e preventivas:**")
    report.append("   - Implemente acompanhamento de a√ß√µes corretivas nos acidentes")
    report.append("   - Verifique encerramento e efic√°cia das a√ß√µes tomadas")
    
    return report

def calculate_forecast(df: pd.DataFrame, months_ahead: int = 1) -> Dict[str, Any]:
    """Calcula previs√µes para os pr√≥ximos meses baseadas em tend√™ncias hist√≥ricas"""
    if df.empty or len(df) < 3:
        return {}
    
    try:
        # Ordena por per√≠odo
        df_sorted = df.sort_values('period').copy()
        
        # Calcula m√©dias m√≥veis dos √∫ltimos 3 meses
        recent_data = df_sorted.tail(3)
        
        # Previs√µes baseadas em m√©dias m√≥veis simples
        forecasts = {}
        
        # Taxa de Frequ√™ncia - usa m√©dia m√≥vel dos √∫ltimos 3 meses
        if 'hours' in df_sorted.columns and 'accidents_total' in df_sorted.columns:
            df_sorted['freq_rate'] = (df_sorted['accidents_total'] / (df_sorted['hours'] * HOURS_SCALE)) * 1_000_000
            
            # Calcula tend√™ncia simples comparando √∫ltimos 3 meses
            recent_freq = df_sorted['freq_rate'].tail(3).values
            if len(recent_freq) >= 2:
                # Tend√™ncia baseada na diferen√ßa entre √∫ltimo e pen√∫ltimo
                trend = recent_freq[-1] - recent_freq[-2] if len(recent_freq) >= 2 else 0
                
                # Previs√£o: √∫ltimo valor + tend√™ncia
                predicted_freq = recent_freq[-1] + trend
                predicted_freq = max(0, predicted_freq)  # N√£o pode ser negativo
                
                forecasts['frequency_rate'] = {
                    'predicted': predicted_freq,
                    'trend': 'increasing' if trend > 0 else 'decreasing' if trend < 0 else 'stable',
                    'confidence': 0.7  # Confian√ßa fixa para simplicidade
                }
        
        # Taxa de Gravidade - usa m√©dia m√≥vel dos √∫ltimos 3 meses
        if 'hours' in df_sorted.columns and 'lost_days_total' in df_sorted.columns:
            df_sorted['sev_rate'] = (df_sorted['lost_days_total'] / (df_sorted['hours'] * HOURS_SCALE)) * 1_000_000
            
            recent_sev = df_sorted['sev_rate'].tail(3).values
            if len(recent_sev) >= 2:
                trend = recent_sev[-1] - recent_sev[-2] if len(recent_sev) >= 2 else 0
                
                predicted_sev = recent_sev[-1] + trend
                predicted_sev = max(0, predicted_sev)  # N√£o pode ser negativo
                
                forecasts['severity_rate'] = {
                    'predicted': predicted_sev,
                    'trend': 'increasing' if trend > 0 else 'decreasing' if trend < 0 else 'stable',
                    'confidence': 0.7
                }
        
        # Total de Acidentes - usa m√©dia m√≥vel dos √∫ltimos 3 meses
        if 'accidents_total' in df_sorted.columns:
            recent_acc = df_sorted['accidents_total'].tail(3).values
            if len(recent_acc) >= 2:
                trend = recent_acc[-1] - recent_acc[-2] if len(recent_acc) >= 2 else 0
                
                predicted_acc = recent_acc[-1] + trend
                predicted_acc = max(0, round(predicted_acc))  # Arredonda e n√£o pode ser negativo
                
                forecasts['total_accidents'] = {
                    'predicted': predicted_acc,
                    'trend': 'increasing' if trend > 0 else 'decreasing' if trend < 0 else 'stable',
                    'confidence': 0.7
                }
        
        # Dias Perdidos - usa m√©dia m√≥vel dos √∫ltimos 3 meses
        if 'lost_days_total' in df_sorted.columns:
            recent_days = df_sorted['lost_days_total'].tail(3).values
            if len(recent_days) >= 2:
                trend = recent_days[-1] - recent_days[-2] if len(recent_days) >= 2 else 0
                
                predicted_days = recent_days[-1] + trend
                predicted_days = max(0, round(predicted_days))  # Arredonda e n√£o pode ser negativo
                
                forecasts['lost_days'] = {
                    'predicted': predicted_days,
                    'trend': 'increasing' if trend > 0 else 'decreasing' if trend < 0 else 'stable',
                    'confidence': 0.7
                }
        
        # Horas trabalhadas (assume valor m√©dio dos √∫ltimos 3 meses)
        if 'hours' in df_sorted.columns:
            avg_hours = float(recent_data['hours'].mean()) * HOURS_SCALE
            forecasts['hours'] = {
                'predicted': round(avg_hours),
                'trend': 'stable',
                'confidence': 0.8
            }
        
        return forecasts
        
    except Exception as e:
        # Em caso de erro, retorna dicion√°rio vazio
        return {}

def generate_forecast_recommendations(forecasts: Dict[str, Any]) -> List[str]:
    """Gera recomenda√ß√µes baseadas nas previs√µes"""
    recommendations = []
    
    # An√°lise da taxa de frequ√™ncia
    if 'frequency_rate' in forecasts:
        freq_data = forecasts['frequency_rate']
        if freq_data['trend'] == 'increasing' and freq_data['predicted'] > 5:
            recommendations.append("üö® **CR√çTICO:** Taxa de frequ√™ncia prevista em alta - implementar medidas preventivas urgentes")
        elif freq_data['trend'] == 'increasing':
            recommendations.append("‚ö†Ô∏è **ATEN√á√ÉO:** Taxa de frequ√™ncia em tend√™ncia de alta - revisar procedimentos")
        elif freq_data['trend'] == 'decreasing':
            recommendations.append("‚úÖ **POSITIVO:** Taxa de frequ√™ncia em tend√™ncia de melhoria - manter pr√°ticas atuais")
    
    # An√°lise da taxa de gravidade
    if 'severity_rate' in forecasts:
        sev_data = forecasts['severity_rate']
        if sev_data['trend'] == 'increasing' and sev_data['predicted'] > 50:
            recommendations.append("üö® **CR√çTICO:** Taxa de gravidade prevista em alta - investigar causas raiz")
        elif sev_data['trend'] == 'increasing':
            recommendations.append("‚ö†Ô∏è **ATEN√á√ÉO:** Taxa de gravidade em tend√™ncia de alta - implementar medidas preventivas")
        elif sev_data['trend'] == 'decreasing':
            recommendations.append("‚úÖ **POSITIVO:** Taxa de gravidade em tend√™ncia de melhoria - documentar boas pr√°ticas")
    
    # An√°lise do total de acidentes
    if 'total_accidents' in forecasts:
        acc_data = forecasts['total_accidents']
        if acc_data['predicted'] > 2:
            recommendations.append("‚ö†Ô∏è **MONITORAR:** Previs√£o de acidentes elevada - intensificar treinamentos")
        elif acc_data['predicted'] == 0:
            recommendations.append("üéØ **META:** Previs√£o de zero acidentes - manter excelente desempenho")
    
    return recommendations

def fetch_detailed_accidents(user_email: str, start_date=None, end_date=None) -> pd.DataFrame:
    """
    Busca dados detalhados de acidentes do usu√°rio atual
    """
    try:
        from managers.supabase_config import get_supabase_client
        from auth.auth_utils import get_user_id
        supabase = get_supabase_client()
        
        # Busca dados de acidentes usando UUID do usu√°rio (created_by agora √© UUID)
        from managers.supabase_config import get_service_role_client
        from auth.auth_utils import is_admin
        user_id = get_user_id()
        if not user_id:
            return pd.DataFrame()
        
        # Admin usa service_role para ver todos os dados, usu√°rio comum usa client normal
        if is_admin():
            supabase = get_service_role_client()
        else:
            supabase = get_supabase_client()
        
        query = supabase.table("accidents").select("*")
        
        # Admin v√™ todos os dados, usu√°rio comum v√™ apenas seus pr√≥prios
        if not is_admin():
            query = query.eq("created_by", user_id)
        
        if start_date:
            query = query.gte("occurred_at", start_date.isoformat())
        if end_date:
            query = query.lte("occurred_at", end_date.isoformat())
            
        response = query.order("occurred_at", desc=True).execute()
        
        if response and hasattr(response, 'data') and response.data:
            return pd.DataFrame(response.data)
        return pd.DataFrame()
        
    except Exception as e:
        st.error(f"Erro ao buscar dados de acidentes: {str(e)}")
        return pd.DataFrame()

def analyze_accidents_by_category(accidents_df: pd.DataFrame) -> Dict[str, Any]:
    """
    Analisa acidentes por categoria (les√£o/fatalidade) e calcula frequ√™ncia
    """
    if accidents_df.empty:
        return {}
    
    # Converte data para datetime se necess√°rio
    if 'occurred_at' in accidents_df.columns:
        accidents_df['occurred_at'] = pd.to_datetime(accidents_df['occurred_at'])
        accidents_df['year_month'] = accidents_df['occurred_at'].dt.to_period('M')
    
    # An√°lise por tipo de acidente
    agg_dict = {
        'id': 'count',
        'lost_days': 'sum'
    }
    
    # is_fatal removido - usar type == 'fatal' em vez disso
    type_analysis = accidents_df.groupby('type').agg(agg_dict).rename(columns={'id': 'count'})
    
    # An√°lise por classifica√ß√£o
    classification_analysis = accidents_df.groupby('classification').agg({
        'id': 'count',
        'lost_days': 'sum'
    }).rename(columns={'id': 'count'})
    
    # An√°lise por parte do corpo
    body_part_analysis = accidents_df.groupby('body_part').agg({
        'id': 'count',
        'lost_days': 'sum'
    }).rename(columns={'id': 'count'})
    
    # An√°lise por causa raiz
    root_cause_analysis = accidents_df.groupby('root_cause').agg({
        'id': 'count',
        'lost_days': 'sum'
    }).rename(columns={'id': 'count'})
    
    # An√°lise temporal (√∫ltimos 12 meses)
    if 'year_month' in accidents_df.columns:
        temporal_agg_dict = {
            'id': 'count',
            'lost_days': 'sum'
        }
        
        # is_fatal removido - usar type == 'fatal' em vez disso
        temporal_analysis = accidents_df.groupby('year_month').agg(temporal_agg_dict).rename(columns={'id': 'count'})
    else:
        temporal_analysis = pd.DataFrame()
    
    # Converte para formato mais leg√≠vel
    def format_analysis(df, analysis_name, group_key=None):
        if df.empty:
            return {}
        
        result = {}
        for idx, row in df.iterrows():
            if pd.isna(idx) or idx == '':
                continue
            
            # Calcula fatalities baseado em type == 'fatal'
            fatalities_count = 0
            if analysis_name == 'by_type':
                # Para an√°lise por tipo, se o √≠ndice for 'fatal', todos s√£o fatais
                fatalities_count = int(row['count']) if str(idx) == 'fatal' else 0
            elif group_key is not None and 'type' in accidents_df.columns:
                # Para outras an√°lises, filtra o dataframe original pelo grupo e conta fatais
                if group_key == 'classification':
                    group_data = accidents_df[accidents_df['classification'] == idx]
                elif group_key == 'body_part':
                    group_data = accidents_df[accidents_df['body_part'] == idx]
                elif group_key == 'root_cause':
                    group_data = accidents_df[accidents_df['root_cause'] == idx]
                elif group_key == 'year_month':
                    group_data = accidents_df[accidents_df['year_month'] == idx]
                else:
                    group_data = pd.DataFrame()
                
                if not group_data.empty:
                    fatalities_count = len(group_data[group_data['type'] == 'fatal'])
            
            result[str(idx)] = {
                'count': int(row['count']),
                'lost_days': int(row.get('lost_days', 0)),
                'fatalities': fatalities_count
            }
        return result
    
    # Calcula total de fatais baseado em type == 'fatal'
    total_fatalities = len(accidents_df[accidents_df['type'] == 'fatal']) if 'type' in accidents_df.columns else 0
    
    return {
        'by_type': format_analysis(type_analysis, 'by_type'),
        'by_classification': format_analysis(classification_analysis, 'by_classification', 'classification'),
        'by_body_part': format_analysis(body_part_analysis, 'by_body_part', 'body_part'),
        'by_root_cause': format_analysis(root_cause_analysis, 'by_root_cause', 'root_cause'),
        'temporal': format_analysis(temporal_analysis, 'temporal', 'year_month'),
        'total_accidents': len(accidents_df),
        'total_fatalities': total_fatalities,
        'total_lost_days': int(accidents_df['lost_days'].sum()) if 'lost_days' in accidents_df.columns else 0,
        'frequency_by_period': calculate_accident_frequency_by_period(accidents_df)
    }

def calculate_accident_frequency_by_period(accidents_df: pd.DataFrame) -> Dict[str, Any]:
    """
    Calcula a frequ√™ncia de acidentes por per√≠odo (mensal)
    """
    if accidents_df.empty or 'occurred_at' not in accidents_df.columns:
        return {}
    
    # Converte data para datetime
    accidents_df['occurred_at'] = pd.to_datetime(accidents_df['occurred_at'])
    accidents_df['year_month'] = accidents_df['occurred_at'].dt.to_period('M')
    
    # Agrupa por per√≠odo e tipo
    period_analysis = accidents_df.groupby(['year_month', 'type']).size().unstack(fill_value=0)
    
    # Calcula frequ√™ncia relativa
    period_totals = period_analysis.sum(axis=1)
    period_frequency = {}
    
    for period in period_analysis.index:
        period_str = str(period)
        period_frequency[period_str] = {
            'total': int(period_totals[period]),
            'fatal': int(period_analysis.loc[period, 'fatal']) if 'fatal' in period_analysis.columns else 0,
            'lesao': int(period_analysis.loc[period, 'lesao']) if 'lesao' in period_analysis.columns else 0,
            'sem_lesao': int(period_analysis.loc[period, 'sem_lesao']) if 'sem_lesao' in period_analysis.columns else 0
        }
    
    return period_frequency